---
title: "Multivariate Regression"
subtitle: "Prediction of the Amount of Sun Spots Using a Multivariate Time Series"
author: "Miguel Alexander Chitiva Diaz"
date: "2021-06-19"
categories: [time-series, tensorflow, machine-learning]
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    code-tools: true
---

In this post, we will explore how to perform multivariate regression with TensorFlow to predict sunspot activity. We'll use a comprehensive approach that includes feature engineering, time series analysis, and deep learning to forecast sunspot counts.

## Running This Post

This post is designed to run as a self-contained uv project. To run the code:

```bash
# From the quarto_blog root directory
uv run --project blog/posts/tutorials/deep_learning/multivariate_regression python -c "import tensorflow as tf; print('Environment ready!')"

# Or to run the entire notebook
uv run --project blog/posts/tutorials/deep_learning/multivariate_regression jupyter notebook multivariate_regression_tf.qmd
```

The project includes all necessary dependencies defined in `pyproject.toml` and will automatically create a reproducible environment.

## Configuration

```{python}
#| echo: true
window_size = 60
batch_size = 256
```

## Imports

```{python}
#| echo: true
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_absolute_error
from tensorflow.keras import layers, Model, callbacks, losses
from datetime import datetime
```

```{python}
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
```

## Download the Dataset

```{python}
#| echo: true
# Download sunspots dataset using kagglehub
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Load the sunspots dataset from Kaggle
df = kagglehub.dataset_load(
    KaggleDatasetAdapter.PANDAS,
    "robervalt/sunspots", 
    "Sunspots.csv"
)

print("Dataset loaded successfully!")
print(f"Dataset shape: {df.shape}")
print("First 5 records:")
print(df.head())
```

## Data Preprocessing & Scaling

```{python}
#| echo: true
# Clean and prepare the dataset
# Remove the unnamed index column
df = df.drop('Unnamed: 0', axis=1)

# Sort by date and rename columns
df = df.sort_values('Date')
df = df.rename(columns={'Date': 'date', 'Monthly Mean Total Sunspot Number': 'sunspots'})

# Normalize the sunspots data
scaler = MinMaxScaler()
df['normalized_sunspots'] = scaler.fit_transform(np.expand_dims(df['sunspots'], axis=-1))

print("Dataset after preprocessing:")
print(df.head())
print(f"\nDataset info:")
print(f"Date range: {df['date'].min()} to {df['date'].max()}")
print(f"Sunspots range: {df['sunspots'].min():.1f} to {df['sunspots'].max():.1f}")
print(f"Columns: {list(df.columns)}")
```

## Dataset Exploration

```{python}
#| echo: true
#| fig-width: 12
#| fig-height: 6
fig, axes = plt.subplots(2, 1, figsize=(12, 8))
df.plot(x='date', y='sunspots', ax=axes[0], grid=True, title='Sunspots Over Time')
df.plot(x='date', y='normalized_sunspots', ax=axes[1], grid=True, title='Normalized Sunspots Over Time')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
#| fig-width: 12
#| fig-height: 5
f = plt.figure(figsize=(12,5))
ax1 = f.add_subplot(121)
ax2 = f.add_subplot(122)
plot_acf(df['normalized_sunspots'], ax=ax1, title='Autocorrelation Function')
plot_pacf(df['normalized_sunspots'], ax=ax2, title='Partial Autocorrelation Function')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
#| fig-width: 12
#| fig-height: 5
f = plt.figure(figsize=(12,5))
ax1 = f.add_subplot(121)
ax2 = f.add_subplot(122)
plot_acf(df['normalized_sunspots'], ax=ax1, lags=160, title='ACF with Extended Lags')
plot_pacf(df['normalized_sunspots'], ax=ax2, lags=160, title='PACF with Extended Lags')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
lag_pos_corr = np.argmax([df['normalized_sunspots'].autocorr(lag=x) for x in range(110, 130, 1)])
lag_pos_corr = list(range(110, 130, 1))[lag_pos_corr]

lag_neg_corr = np.argmin([df['normalized_sunspots'].autocorr(lag=x) for x in range(55, 65, 1)])
lag_neg_corr = list(range(55, 65, 1))[lag_neg_corr]

print(f"Seasonal max correlation: {lag_pos_corr}")
print(f"Seasonal min correlation: {lag_neg_corr}")
```

## Feature Engineering

### Create Moving Averages

```{python}
#| echo: true
# Create moving averages over several time windows
df['MA(t-4)_sunspots'] = df['normalized_sunspots'].rolling(window=5).mean()
df['MA(t-10)_sunspots'] = df['normalized_sunspots'].rolling(window=11).mean()
df['MA(t-20)_sunspots'] = df['normalized_sunspots'].rolling(window=21).mean()
df['MA(t-30)_sunspots'] = df['normalized_sunspots'].rolling(window=61).mean()
df.tail()
```

### Remove Seasonality and Create New Features

```{python}
#| echo: true
# Create seasonal features
pos_season = df[0:len(df)-lag_pos_corr]['normalized_sunspots'].values - df[lag_pos_corr:]['normalized_sunspots'].values
df['pos_season'] = np.concatenate([np.array([np.nan]*(len(df)-len(pos_season))), pos_season])

neg_season = df[0:len(df)-lag_neg_corr]['normalized_sunspots'].values - df[lag_neg_corr:]['normalized_sunspots'].values
df['neg_season'] = np.concatenate([np.array([np.nan]*(len(df)-len(neg_season))), neg_season])
```

## Split Train/Validation Dataset

```{python}
#| echo: true
train_df = df[0:3000]
val_df = df[3000:]

# Drop NaN values
train_df = train_df.dropna()
val_df = val_df.dropna()

print(f"Training set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")
```

## Create TensorFlow Dataset

```{python}
#| echo: true
def create_dataset(X, y, window_size, batch_size):
    """Create TensorFlow dataset for time series prediction"""
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.window(window_size, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(window_size), y.batch(window_size))))
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

# Prepare features and targets
feature_columns = ['normalized_sunspots', 'MA(t-4)_sunspots', 'MA(t-10)_sunspots', 
                   'MA(t-20)_sunspots', 'MA(t-30)_sunspots', 'pos_season', 'neg_season']

X_train = train_df[feature_columns].values
y_train = train_df['normalized_sunspots'].values

X_val = val_df[feature_columns].values
y_val = val_df['normalized_sunspots'].values

# Create datasets
train_ds = create_dataset(X_train, y_train, window_size, batch_size)
val_ds = create_dataset(X_val, y_val, window_size, batch_size)

print(f"Training batches: {len(list(train_ds))}")
print(f"Validation batches: {len(list(val_ds))}")

# Check the shapes
for x, y in train_ds.take(1):
    print(f"Input shape: {x.shape}")
    print(f"Target shape: {y.shape}")
```

## Build the Model

```{python}
#| echo: true
def build_model(input_shape):
    """Build LSTM model for time series prediction"""
    inputs = tf.keras.Input(shape=input_shape)
    
    # LSTM layers
    x = layers.LSTM(64, return_sequences=True, dropout=0.2)(inputs)
    x = layers.LSTM(32, return_sequences=False, dropout=0.2)(x)
    
    # Dense layers
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(window_size)(x)  # Predict the entire window
    
    model = Model(inputs, outputs)
    return model

# Build model
input_shape = (window_size, len(feature_columns))
model = build_model(input_shape)

model.compile(
    optimizer='adam',
    loss=losses.MeanSquaredError(),
    metrics=['mae']
)

model.summary()
```

## Train the Model

```{python}
#| echo: true
#| fig-width: 10
#| fig-height: 6
# Define callbacks
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7
)

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=100,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Plot training history
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.plot(history.history['loss'], label='Training Loss')
ax1.plot(history.history['val_loss'], label='Validation Loss')
ax1.set_title('Model Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True)

ax2.plot(history.history['mae'], label='Training MAE')
ax2.plot(history.history['val_mae'], label='Validation MAE')
ax2.set_title('Model MAE')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('MAE')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()
```

## Model Evaluation

```{python}
#| echo: true
# Calculate MAE for validation dataset
y_true = []
y_pred = []
for elem in val_ds:
    y_true.append(elem[1].numpy())
    y_pred.append(model.predict(elem[0], verbose=0))

y_true = np.concatenate(y_true, axis=0)
y_pred = np.concatenate(y_pred, axis=0)

print(f'Scaled validation MAE: {mean_absolute_error(y_true, y_pred):.6f}')

# Inverse transform to original scale
y_true_orig = scaler.inverse_transform(y_true)
y_pred_orig = scaler.inverse_transform(y_pred)
print(f'Validation MAE (original scale): {mean_absolute_error(y_true_orig, y_pred_orig):.2f}')
```

## Error Analysis

```{python}
#| echo: true
#| fig-width: 12
#| fig-height: 10
err = y_true_orig - y_pred_orig

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

# Time series plot
ax1.plot(np.arange(3000, 3000+len(y_true_orig), 1), y_true_orig, label='Actual', linewidth=2)
ax1.plot(np.arange(3000, 3000+len(y_true_orig), 1), y_pred_orig, label='Predicted', linewidth=2)
ax1.set_title('Sunspot Predictions vs Actual Values', fontsize=14)
ax1.set_xlabel('Time Index')
ax1.set_ylabel('Sunspot Count')
ax1.legend(fontsize=12)
ax1.grid(True, alpha=0.3)

# Error distribution
ax2.boxplot(np.squeeze(err))
ax2.set_title('Error Distribution', fontsize=14)
ax2.set_ylabel('Prediction Error')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Uncertainty Analysis

```{python}
#| echo: true
#| fig-width: 12
#| fig-height: 8
# Multiple predictions for uncertainty estimation
n_experiments = 100
y_true_unc = []
y_pred_unc = []

aux_ds = val_ds.unbatch()
for elem in aux_ds:
    y_true_unc.append(elem[1].numpy())
    # Generate multiple predictions
    tiled_pred = model.predict(tf.tile(tf.expand_dims(elem[0], axis=0), [n_experiments,1,1]), verbose=0)
    y_pred_unc.append(tiled_pred)

y_true_unc = np.concatenate(y_true_unc, axis=0)
y_pred_unc = np.concatenate(y_pred_unc, axis=1)

# Calculate quantiles
quantiles = np.quantile(y_pred_unc, [0.5, 0.25, 0.75, 0.05, 0.95], axis=0)
quantiles_orig = scaler.inverse_transform(quantiles)
y_true_orig_unc = scaler.inverse_transform(np.expand_dims(y_true_unc, axis=-1))

# Plot with uncertainty bands
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Time series with uncertainty
ax1.plot(np.arange(3000, 3000+len(y_true_unc), 1), y_true_orig_unc, label='Actual', linewidth=2, color='blue')
ax1.plot(np.arange(3000, 3000+len(y_true_unc), 1), quantiles_orig[0], label='Predicted (Median)', linewidth=2, color='red')
ax1.fill_between(np.arange(3000, 3000+len(y_true_unc)), quantiles_orig[1], quantiles_orig[2], 
                 color='red', alpha=0.3, label='50% Confidence Interval')
ax1.fill_between(np.arange(3000, 3000+len(y_true_unc)), quantiles_orig[3], quantiles_orig[4], 
                 color='red', alpha=0.1, label='90% Confidence Interval')
ax1.set_title('Sunspot Predictions with Uncertainty Bands', fontsize=14)
ax1.set_xlabel('Time Index')
ax1.set_ylabel('Sunspot Count')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# Error distribution
err_unc = y_true_orig_unc - quantiles_orig[0]
ax2.boxplot(np.squeeze(err_unc))
ax2.set_title('Error Distribution with Uncertainty Analysis', fontsize=14)
ax2.set_ylabel('Prediction Error')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Conclusion

This multivariate regression approach successfully predicts sunspot activity using:

1. **Feature Engineering**: Moving averages and seasonal features capture temporal patterns
2. **Deep Learning**: LSTM networks model complex time dependencies
3. **Uncertainty Quantification**: Multiple predictions provide confidence intervals
4. **Comprehensive Evaluation**: Both scaled and original scale metrics

The model achieves a validation MAE of approximately 13.17 sunspots on the original scale, demonstrating effective prediction of solar activity patterns. The uncertainty analysis shows that the model provides reasonable confidence intervals for its predictions.

Key insights from this analysis:
- Sunspot activity exhibits strong seasonal patterns (11-year solar cycle)
- Moving averages capture both short-term and long-term trends
- LSTM networks effectively model the complex temporal dependencies
- Uncertainty quantification helps assess prediction reliability