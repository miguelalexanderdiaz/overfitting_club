{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multivariate Regression\"\n",
    "subtitle: \"Prediction of the Amount of Sun Spots Using a Multivariate Time Series\"\n",
    "author: \"Miguel Alexander Chitiva Diaz\"\n",
    "date: \"2021-06-19\"\n",
    "categories: [time-series, tensorflow, machine-learning]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "---\n",
    "\n",
    "Quick test, nothing to see here. ABC\n",
    "\n",
    "In this post, we will explore how to perform multivariate regression with TensorFlow to predict sunspot activity. We'll use a comprehensive approach that includes feature engineering, time series analysis, and deep learning to forecast sunspot counts.\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install uv package manager\n!pip install -q uv\n\n# Download pyproject.toml and uv.lock from GitHub\n!curl -sO https://raw.githubusercontent.com/miguelalexanderdiaz/quarto_blog/main/blog/posts/tutorials/deep_learning/multivariate_regression/pyproject.toml\n!curl -sO https://raw.githubusercontent.com/miguelalexanderdiaz/quarto_blog/main/blog/posts/tutorials/deep_learning/multivariate_regression/uv.lock\n\n# Install dependencies system-wide using uv (respects lock file for exact reproducibility)\n!uv pip install --system -r pyproject.toml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "window_size = 60\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    module=r'^keras(\\.|$)'\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras import layers, Model, callbacks, losses\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "# Load the sunspots dataset from Kaggle\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"robervalt/sunspots\", \n",
    "    \"Sunspots.csv\",\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.sort_values('Date')\n",
    "df = df.rename(columns={df.columns[0]: 'date', df.columns[1]: 'sunspots'})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['normalized_sunspots'] = scaler.fit_transform(np.expand_dims(df['sunspots'], axis=-1))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "fig-height": 10,
    "fig-width": 150
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "graphs = df.plot(grid=True, figsize=(50,10), fontsize=20, subplots=True)\n",
    "[x.legend(prop={'size': 30}) for x in graphs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "fig-height": 5,
    "fig-width": 12
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "f = plt.figure(figsize=(12,5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "plot_acf(df['normalized_sunspots'], ax=ax1, title='Autocorrelation Function')\n",
    "plot_pacf(df['normalized_sunspots'], ax=ax2, title='Partial Autocorrelation Function')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "fig-height": 5,
    "fig-width": 12
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "f = plt.figure(figsize=(12,5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "plot_acf(df['normalized_sunspots'], ax=ax1, lags=160, title='ACF with Extended Lags')\n",
    "plot_pacf(df['normalized_sunspots'], ax=ax2, lags=160, title='PACF with Extended Lags')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "lag_pos_corr = np.argmax([df['normalized_sunspots'].autocorr(lag=x) for x in range(110, 130, 1)])\n",
    "lag_pos_corr = list(range(110, 130, 1))[lag_pos_corr]\n",
    "\n",
    "lag_neg_corr = np.argmin([df['normalized_sunspots'].autocorr(lag=x) for x in range(55, 65, 1)])\n",
    "lag_neg_corr = list(range(55, 65, 1))[lag_neg_corr]\n",
    "\n",
    "print(f\"Seasonal max correlation: {lag_pos_corr}\")\n",
    "print(f\"Seasonal min correlation: {lag_neg_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "lag_pos_corr = np.argmax([df['normalized_sunspots'].autocorr(lag=x) for x in range(110, 130, 1) ])\n",
    "lag_pos_corr = list(range(110, 130, 1))[lag_pos_corr]\n",
    "\n",
    "lag_neg_corr = np.argmin([df['normalized_sunspots'].autocorr(lag=x) for x in range(55, 65, 1) ])\n",
    "lag_neg_corr = list(range(55, 65, 1))[lag_neg_corr]\n",
    "\n",
    "print(f\"seasonal max_corr:{lag_pos_corr} \\t seasonal min_corr:{lag_neg_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Create Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "# Create moving averages over several time windows\n",
    "df['MA(t-4)_sunspots'] = df['normalized_sunspots'].rolling(window=5).mean()\n",
    "df['MA(t-10)_sunspots'] = df['normalized_sunspots'].rolling(window=11).mean()\n",
    "df['MA(t-20)_sunspots'] = df['normalized_sunspots'].rolling(window=21).mean()\n",
    "df['MA(t-30)_sunspots'] = df['normalized_sunspots'].rolling(window=61).mean()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Seasonality and Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "# Create seasonal features\n",
    "pos_season = df[0:len(df)-lag_pos_corr]['normalized_sunspots'].values - df[lag_pos_corr:]['normalized_sunspots'].values\n",
    "df['pos_season'] = np.concatenate([np.array([np.nan]*(len(df)-len(pos_season))), pos_season])\n",
    "\n",
    "neg_season = df[0:len(df)-lag_neg_corr]['normalized_sunspots'].values - df[lag_neg_corr:]['normalized_sunspots'].values\n",
    "df['neg_season'] = np.concatenate([np.array([np.nan]*(len(df)-len(neg_season))), neg_season])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "train_df = df[0:3000]\n",
    "val_df = df[3000:]\n",
    "\n",
    "# Drop NaN values\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Multivariate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "val_df[val_df.columns[-7:]].plot(figsize=(50,15), fontsize=20, grid=True).legend(prop={'size': 20});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Predictions using only the created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "for col in val_df.columns[-6:]:\n",
    "    print(f\"MAE using only '{col}' = {mean_absolute_error(val_df['normalized_sunspots'], val_df[col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Model and Dataset \n",
    "\n",
    "### Create Windowed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "train_ds  = tf.data.Dataset.from_tensor_slices(train_df[train_df.columns[-7:]])\n",
    "val_ds  = tf.data.Dataset.from_tensor_slices(val_df[val_df.columns[-7:]])\n",
    "\n",
    "\n",
    "def windowed_ds(ds, window_size, pred_window=1, shift=1):\n",
    "    ds = ds.window(window_size + pred_window, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + pred_window))\n",
    "    ds = ds.map(lambda w: (w[:-pred_window],w[-pred_window:][:,0]))\n",
    "    return ds\n",
    "\n",
    "train_ds = windowed_ds(train_ds, window_size).repeat(10).shuffle(500, reshuffle_each_iteration=True).batch(batch_size)\n",
    "val_ds = windowed_ds(val_ds, window_size).batch(batch_size).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model (CNN + RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(window_size,7))\n",
    "x = layers.Conv1D(filters=32, kernel_size=10, padding='causal')(inputs)\n",
    "x = layers.Conv1D(filters=64, kernel_size=2)(x)\n",
    "x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(128)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x, training=True)\n",
    "x = layers.Dense(1)(x)\n",
    "model = Model(inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.Huber(), optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor='val_mae', min_delta=1e-3, patience=30, mode='min', restore_best_weights=True)\n",
    "checkpoint = callbacks.ModelCheckpoint(f'models/sunspots.model.keras', monitor='mae', save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=[early_stop, checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for elem in val_ds:\n",
    "    y_true.append(elem[1].numpy())\n",
    "    y_pred.append(model.predict(elem[0]))\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "print('scaled val_MAE:', mean_absolute_error(y_true, y_pred))\n",
    "y_true = scaler.inverse_transform(y_true)\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "print('val_MAE:', mean_absolute_error(y_true, y_pred))\n",
    "err = y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "f = plt.figure(figsize=(50,15))\n",
    "ax1 = f.add_subplot(211)\n",
    "ax2 = f.add_subplot(212)\n",
    "ax1.plot(np.arange(3000, 3000+len(y_true), 1), y_true)\n",
    "ax1.plot(np.arange(3000, 3000+len(y_true), 1), y_pred)\n",
    "ax1.grid()\n",
    "\n",
    "sns.boxplot(np.squeeze(err), ax=ax2, orient='h')\n",
    "ax1.legend(['y_true', 'y_pred'], prop={'size': 30})\n",
    "ax2.set_title('total error distribution', fontsize=30)\n",
    "ax1.xaxis.set_tick_params(labelsize=20)\n",
    "ax1.yaxis.set_tick_params(labelsize=20)\n",
    "ax2.xaxis.set_tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "n_experiments = 300\n",
    "y_true = []\n",
    "y_pred = []\n",
    "aux_ds = val_ds.unbatch()\n",
    "for elem in aux_ds:\n",
    "    y_true.append(elem[1].numpy())\n",
    "    # Vectorized MC dropout: tile the input and call the model once with training=True\n",
    "    tiled_inputs = tf.tile(tf.expand_dims(elem[0], axis=0), [n_experiments, 1, 1])\n",
    "    preds = model(tiled_inputs)\n",
    "    y_pred.append(preds)\n",
    "\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.quantile(y_pred, [0.5,0.25,0.75,0.05,0.95], axis=0)\n",
    "quantiles = scaler.inverse_transform(quantiles)\n",
    "quantiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "y_true_tr = scaler.inverse_transform(np.expand_dims(y_true, axis=-1))\n",
    "y_pred_tr = np.expand_dims(quantiles[0], axis=-1)\n",
    "err = (y_true_tr - y_pred_tr).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "\n",
    "f = plt.figure(figsize=(50,15))\n",
    "ax1 = f.add_subplot(211)\n",
    "ax2 = f.add_subplot(212)\n",
    "ax1.plot(np.arange(3000, 3000+len(y_true), 1), y_true_tr)\n",
    "ax1.plot(np.arange(3000, 3000+len(y_true), 1), quantiles[0])\n",
    "ax1.fill_between(np.arange(3000, 3000+len(y_true)), quantiles[1], quantiles[2], color='b', alpha=.2)\n",
    "ax1.fill_between(np.arange(3000, 3000+len(y_true)), quantiles[3], quantiles[4], color='b', alpha=.1)\n",
    "ax1.grid()\n",
    "sns.boxplot(x=err, ax=ax2, orient='h')\n",
    "ax1.legend(['y_true', 'y_pred'], prop={'size': 30})\n",
    "ax2.set_title('total error distribution', fontsize=30)\n",
    "ax1.xaxis.set_tick_params(labelsize=20)\n",
    "ax1.yaxis.set_tick_params(labelsize=20)\n",
    "ax2.xaxis.set_tick_params(labelsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/migueldiaz/.anaconda/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}